{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "UNSW-NB15 Network Packet Classification - PyTorch Categorical Variational AutoEncoder with Gumbel-Softmax.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alik604/cyber-security/blob/master/Intrusion-Detection/UNSW_NB15_Network_Packet_Classification_PyTorch_Categorical_Variational_AutoEncoder_with_Gumbel_Softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NZuC1Fk8t7t"
      },
      "source": [
        "%config IPCompleter.greedy=True\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import sklearn\n",
        "from sklearn.decomposition import * \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import *\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "\n",
        "\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "DLed = False"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1xx6RqX8t71",
        "outputId": "4c239c06-a651-4106-ed5d-4d4b48b2ece4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "if not DLed:\n",
        "    train = pd.read_csv('https://raw.githubusercontent.com/Nir-J/ML-Projects/master/UNSW-Network_Packet_Classification/UNSW_NB15_training-set.csv')\n",
        "    test = pd.read_csv('https://raw.githubusercontent.com/Nir-J/ML-Projects/master/UNSW-Network_Packet_Classification/UNSW_NB15_testing-set.csv')\n",
        "    DLed = True\n",
        "    \n",
        "combined_data = pd.concat([train, test]).drop(['id','label'],1)\n",
        "combined_data.shape\n",
        "combined_data.head(5)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(257673, 43)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dur</th>\n",
              "      <th>proto</th>\n",
              "      <th>service</th>\n",
              "      <th>state</th>\n",
              "      <th>spkts</th>\n",
              "      <th>dpkts</th>\n",
              "      <th>sbytes</th>\n",
              "      <th>dbytes</th>\n",
              "      <th>rate</th>\n",
              "      <th>sttl</th>\n",
              "      <th>dttl</th>\n",
              "      <th>sload</th>\n",
              "      <th>dload</th>\n",
              "      <th>sloss</th>\n",
              "      <th>dloss</th>\n",
              "      <th>sinpkt</th>\n",
              "      <th>dinpkt</th>\n",
              "      <th>sjit</th>\n",
              "      <th>djit</th>\n",
              "      <th>swin</th>\n",
              "      <th>stcpb</th>\n",
              "      <th>dtcpb</th>\n",
              "      <th>dwin</th>\n",
              "      <th>tcprtt</th>\n",
              "      <th>synack</th>\n",
              "      <th>ackdat</th>\n",
              "      <th>smean</th>\n",
              "      <th>dmean</th>\n",
              "      <th>trans_depth</th>\n",
              "      <th>response_body_len</th>\n",
              "      <th>ct_srv_src</th>\n",
              "      <th>ct_state_ttl</th>\n",
              "      <th>ct_dst_ltm</th>\n",
              "      <th>ct_src_dport_ltm</th>\n",
              "      <th>ct_dst_sport_ltm</th>\n",
              "      <th>ct_dst_src_ltm</th>\n",
              "      <th>is_ftp_login</th>\n",
              "      <th>ct_ftp_cmd</th>\n",
              "      <th>ct_flw_http_mthd</th>\n",
              "      <th>ct_src_ltm</th>\n",
              "      <th>ct_srv_dst</th>\n",
              "      <th>is_sm_ips_ports</th>\n",
              "      <th>attack_cat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.121478</td>\n",
              "      <td>tcp</td>\n",
              "      <td>-</td>\n",
              "      <td>FIN</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>258</td>\n",
              "      <td>172</td>\n",
              "      <td>74.087490</td>\n",
              "      <td>252</td>\n",
              "      <td>254</td>\n",
              "      <td>14158.942380</td>\n",
              "      <td>8495.365234</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24.295600</td>\n",
              "      <td>8.375000</td>\n",
              "      <td>30.177547</td>\n",
              "      <td>11.830604</td>\n",
              "      <td>255</td>\n",
              "      <td>621772692</td>\n",
              "      <td>2202533631</td>\n",
              "      <td>255</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.649902</td>\n",
              "      <td>tcp</td>\n",
              "      <td>-</td>\n",
              "      <td>FIN</td>\n",
              "      <td>14</td>\n",
              "      <td>38</td>\n",
              "      <td>734</td>\n",
              "      <td>42014</td>\n",
              "      <td>78.473372</td>\n",
              "      <td>62</td>\n",
              "      <td>252</td>\n",
              "      <td>8395.112305</td>\n",
              "      <td>503571.312500</td>\n",
              "      <td>2</td>\n",
              "      <td>17</td>\n",
              "      <td>49.915000</td>\n",
              "      <td>15.432865</td>\n",
              "      <td>61.426934</td>\n",
              "      <td>1387.778330</td>\n",
              "      <td>255</td>\n",
              "      <td>1417884146</td>\n",
              "      <td>3077387971</td>\n",
              "      <td>255</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>52</td>\n",
              "      <td>1106</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.623129</td>\n",
              "      <td>tcp</td>\n",
              "      <td>-</td>\n",
              "      <td>FIN</td>\n",
              "      <td>8</td>\n",
              "      <td>16</td>\n",
              "      <td>364</td>\n",
              "      <td>13186</td>\n",
              "      <td>14.170161</td>\n",
              "      <td>62</td>\n",
              "      <td>252</td>\n",
              "      <td>1572.271851</td>\n",
              "      <td>60929.230470</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>231.875571</td>\n",
              "      <td>102.737203</td>\n",
              "      <td>17179.586860</td>\n",
              "      <td>11420.926230</td>\n",
              "      <td>255</td>\n",
              "      <td>2116150707</td>\n",
              "      <td>2963114973</td>\n",
              "      <td>255</td>\n",
              "      <td>0.111897</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>0.050439</td>\n",
              "      <td>46</td>\n",
              "      <td>824</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.681642</td>\n",
              "      <td>tcp</td>\n",
              "      <td>ftp</td>\n",
              "      <td>FIN</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>628</td>\n",
              "      <td>770</td>\n",
              "      <td>13.677108</td>\n",
              "      <td>62</td>\n",
              "      <td>252</td>\n",
              "      <td>2740.178955</td>\n",
              "      <td>3358.622070</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>152.876547</td>\n",
              "      <td>90.235726</td>\n",
              "      <td>259.080172</td>\n",
              "      <td>4991.784669</td>\n",
              "      <td>255</td>\n",
              "      <td>1107119177</td>\n",
              "      <td>1047442890</td>\n",
              "      <td>255</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>52</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.449454</td>\n",
              "      <td>tcp</td>\n",
              "      <td>-</td>\n",
              "      <td>FIN</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>534</td>\n",
              "      <td>268</td>\n",
              "      <td>33.373826</td>\n",
              "      <td>254</td>\n",
              "      <td>252</td>\n",
              "      <td>8561.499023</td>\n",
              "      <td>3987.059814</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>47.750333</td>\n",
              "      <td>75.659602</td>\n",
              "      <td>2415.837634</td>\n",
              "      <td>115.807000</td>\n",
              "      <td>255</td>\n",
              "      <td>2436137549</td>\n",
              "      <td>1977154190</td>\n",
              "      <td>255</td>\n",
              "      <td>0.128381</td>\n",
              "      <td>0.071147</td>\n",
              "      <td>0.057234</td>\n",
              "      <td>53</td>\n",
              "      <td>45</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>39</td>\n",
              "      <td>0</td>\n",
              "      <td>Normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        dur proto service  ... ct_srv_dst  is_sm_ips_ports  attack_cat\n",
              "0  0.121478   tcp       -  ...          1                0      Normal\n",
              "1  0.649902   tcp       -  ...          6                0      Normal\n",
              "2  1.623129   tcp       -  ...          6                0      Normal\n",
              "3  1.681642   tcp     ftp  ...          1                0      Normal\n",
              "4  0.449454   tcp       -  ...         39                0      Normal\n",
              "\n",
              "[5 rows x 43 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFWVaeB18t7-",
        "outputId": "8fb2262a-643c-461c-e281-d75757530c8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tmp = train.where(train['attack_cat'] == \"Normal\").dropna()\n",
        "print('Train ', round(len(tmp['attack_cat'])/len(train['attack_cat']),5))\n",
        "\n",
        "tmp = test.where(test['attack_cat'] == \"Normal\").dropna()\n",
        "print('Test ', round(len(tmp['attack_cat'])/len(test['attack_cat']),5))\n",
        "\n",
        "# del train, test"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train  0.31938\n",
            "Test  0.4494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAIu3z8U8t8e",
        "outputId": "8a0d6bec-aa6a-4773-ad5e-9968fff72cbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "le = LabelEncoder()\n",
        "\n",
        "vector = combined_data['attack_cat']\n",
        "print(\"attack cats:\", list(set(list(vector)))) # use print to make it print on single line \n",
        "\n",
        "combined_data['attack_cat'] = le.fit_transform(vector)\n",
        "combined_data['proto'] = le.fit_transform(combined_data['proto'])\n",
        "combined_data['service'] = le.fit_transform(combined_data['service'])\n",
        "combined_data['state'] = le.fit_transform(combined_data['state'])\n",
        "\n",
        "print('\\nDescribing attack types: ')\n",
        "\n",
        "print(\"mode\", vector.mode())\n",
        "print(f\"mode {np.sum(combined_data['attack_cat'].values==6)/vector.shape[0]:.2f}%\") # alt numerator:  len(np.where(combined_data['attack_cat'].values==6)[0])\n",
        "\n",
        "print(\"looks like 6 is 'normal', but its not that common\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attack cats: ['Worms', 'Shellcode', 'Analysis', 'Backdoor', 'DoS', 'Fuzzers', 'Normal', 'Exploits', 'Generic', 'Reconnaissance']\n",
            "\n",
            "Describing attack types: \n",
            "mode 0    Normal\n",
            "dtype: object\n",
            "mode 0.36%\n",
            "looks like 6 is 'normal', but its not that common\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kZ7v4Ix8t8i",
        "outputId": "020b3122-eb67-4452-cc38-411af57e3672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "import collections\n",
        "from tabulate import tabulate\n",
        "\n",
        "counter = collections.Counter(vector)  \n",
        "print(tabulate(counter.most_common(),headers = ['Type','Occurences']))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type              Occurences\n",
            "--------------  ------------\n",
            "Normal                 93000\n",
            "Generic                58871\n",
            "Exploits               44525\n",
            "Fuzzers                24246\n",
            "DoS                    16353\n",
            "Reconnaissance         13987\n",
            "Analysis                2677\n",
            "Backdoor                2329\n",
            "Shellcode               1511\n",
            "Worms                    174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6sd0tUT8t8l"
      },
      "source": [
        "COPY = combined_data.copy(deep=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNVXJaYb8t8p"
      },
      "source": [
        "combined_data = COPY\n",
        "assert COPY.shape == combined_data.shape\n",
        "\n",
        "lowSTD = list(combined_data.std().to_frame().nsmallest(7, columns=0).index)\n",
        "lowCORR = list(combined_data.corr().abs().sort_values('attack_cat')['attack_cat'].nsmallest(7).index) # .where(lambda x: x < 0.005).dropna()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q0WI4LX8t8v",
        "outputId": "eacb7780-bd13-4b85-f5b2-024c24b597e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "exclude = list(lowCORR + lowSTD)\n",
        "if 'attack_cat' in exclude:\n",
        "  exclude.remove('attack_cat')\n",
        "\n",
        "print('shape before:', combined_data.shape)\n",
        "print('replace the following with their PCA(3) -', exclude)\n",
        "\n",
        "# dim_reduct = SparsePCA(10, n_jobs = -1).fit_transform(combined_data)\n",
        "# dim_reduct = TruncatedSVD(4).fit_transform(combined_data[exclude])\n",
        "pca = PCA(3)\n",
        "dim_reduct = pca.fit_transform(combined_data[exclude])\n",
        "\n",
        "print(\"explained_variance_ratio_ is\", sum(pca.explained_variance_ratio_))\n",
        "\n",
        "combined_data.drop(exclude,axis=1,inplace=True)\n",
        "\n",
        "dim_reduction = pd.DataFrame(dim_reduct)\n",
        "combined_data = combined_data.join(dim_reduction)\n",
        "\n",
        "print('shape after:', combined_data.shape)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape before: (257673, 43)\n",
            "replace the following with their PCA(3) - ['sjit', 'response_body_len', 'djit', 'dinpkt', 'dbytes', 'spkts', 'dloss', 'ackdat', 'synack', 'tcprtt', 'is_ftp_login', 'ct_ftp_cmd', 'is_sm_ips_ports', 'ct_flw_http_mthd']\n",
            "explained_variance_ratio_ is 0.999538282884239\n",
            "shape after: (257673, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMc5Eklk3-YX",
        "outputId": "6e13024d-473e-4ca1-ce53-958e3788c8b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "source": [
        "print('combined_data.dur is scaled up by 10,000')\n",
        "combined_data['dur'] = 10000*combined_data['dur']\n",
        "combined_data.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "combined_data.dur is scaled up by 10,000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dur</th>\n",
              "      <th>proto</th>\n",
              "      <th>service</th>\n",
              "      <th>state</th>\n",
              "      <th>dpkts</th>\n",
              "      <th>sbytes</th>\n",
              "      <th>rate</th>\n",
              "      <th>sttl</th>\n",
              "      <th>dttl</th>\n",
              "      <th>sload</th>\n",
              "      <th>dload</th>\n",
              "      <th>sloss</th>\n",
              "      <th>sinpkt</th>\n",
              "      <th>swin</th>\n",
              "      <th>stcpb</th>\n",
              "      <th>dtcpb</th>\n",
              "      <th>dwin</th>\n",
              "      <th>smean</th>\n",
              "      <th>dmean</th>\n",
              "      <th>trans_depth</th>\n",
              "      <th>ct_srv_src</th>\n",
              "      <th>ct_state_ttl</th>\n",
              "      <th>ct_dst_ltm</th>\n",
              "      <th>ct_src_dport_ltm</th>\n",
              "      <th>ct_dst_sport_ltm</th>\n",
              "      <th>ct_dst_src_ltm</th>\n",
              "      <th>ct_src_ltm</th>\n",
              "      <th>ct_srv_dst</th>\n",
              "      <th>attack_cat</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1214.78</td>\n",
              "      <td>113</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>258</td>\n",
              "      <td>74.087490</td>\n",
              "      <td>252</td>\n",
              "      <td>254</td>\n",
              "      <td>1.415894e+04</td>\n",
              "      <td>8495.365234</td>\n",
              "      <td>0</td>\n",
              "      <td>24.295600</td>\n",
              "      <td>255</td>\n",
              "      <td>621772692</td>\n",
              "      <td>2202533631</td>\n",
              "      <td>255</td>\n",
              "      <td>43</td>\n",
              "      <td>43</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-14339.811230</td>\n",
              "      <td>-5427.117131</td>\n",
              "      <td>363.455824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.11</td>\n",
              "      <td>119</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>496</td>\n",
              "      <td>90909.090200</td>\n",
              "      <td>254</td>\n",
              "      <td>0</td>\n",
              "      <td>1.803636e+08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.011000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>248</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>-14339.811230</td>\n",
              "      <td>-5427.117131</td>\n",
              "      <td>363.455824</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6499.02</td>\n",
              "      <td>113</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>38</td>\n",
              "      <td>734</td>\n",
              "      <td>78.473372</td>\n",
              "      <td>62</td>\n",
              "      <td>252</td>\n",
              "      <td>8.395112e+03</td>\n",
              "      <td>503571.312500</td>\n",
              "      <td>2</td>\n",
              "      <td>49.915000</td>\n",
              "      <td>255</td>\n",
              "      <td>1417884146</td>\n",
              "      <td>3077387971</td>\n",
              "      <td>255</td>\n",
              "      <td>52</td>\n",
              "      <td>1106</td>\n",
              "      <td>0</td>\n",
              "      <td>43</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>26947.677836</td>\n",
              "      <td>-5283.049895</td>\n",
              "      <td>-6435.428556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.08</td>\n",
              "      <td>119</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1762</td>\n",
              "      <td>125000.000300</td>\n",
              "      <td>254</td>\n",
              "      <td>0</td>\n",
              "      <td>8.810000e+08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>881</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>26947.677836</td>\n",
              "      <td>-5283.049895</td>\n",
              "      <td>-6435.428556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16231.29</td>\n",
              "      <td>113</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>364</td>\n",
              "      <td>14.170161</td>\n",
              "      <td>62</td>\n",
              "      <td>252</td>\n",
              "      <td>1.572272e+03</td>\n",
              "      <td>60929.230470</td>\n",
              "      <td>1</td>\n",
              "      <td>231.875571</td>\n",
              "      <td>255</td>\n",
              "      <td>2116150707</td>\n",
              "      <td>2963114973</td>\n",
              "      <td>255</td>\n",
              "      <td>46</td>\n",
              "      <td>824</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>-1508.847447</td>\n",
              "      <td>12187.598091</td>\n",
              "      <td>-1747.491867</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        dur  proto  service  ...             0             1            2\n",
              "0   1214.78    113        0  ... -14339.811230  -5427.117131   363.455824\n",
              "0      0.11    119        0  ... -14339.811230  -5427.117131   363.455824\n",
              "1   6499.02    113        0  ...  26947.677836  -5283.049895 -6435.428556\n",
              "1      0.08    119        0  ...  26947.677836  -5283.049895 -6435.428556\n",
              "2  16231.29    113        0  ...  -1508.847447  12187.598091 -1747.491867\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVdK_9Wh8t82",
        "outputId": "4eae2907-c1be-493b-9fc2-64017b8a1536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print('before:', combined_data.shape)\n",
        "data_x = combined_data.drop(['attack_cat'], axis=1) # droped label\n",
        "data_y = combined_data['attack_cat']\n",
        "data_x.shape\n",
        "data_y.shape\n",
        "\n",
        "data_x = data_x.apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
        "\n",
        "# data_x = MinMaxScaler().fit_transform(data_x) # better for VotingClassifier\n",
        "# data_x = StandardScaler().fit_transform(data_x)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: (257673, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(257673, 31)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(257673,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aig47qjH8t85"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=.50, random_state=42) # TODO"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3sNcsgr8t89",
        "outputId": "d1dcaf8e-523c-425f-ca9f-3efa21b12b9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "DTC = DecisionTreeClassifier() \n",
        "RFC = RandomForestClassifier(n_estimators=50, random_state=1)\n",
        "ETC = ExtraTreesClassifier(n_estimators=75, criterion='gini', max_features='auto', bootstrap=False)\n",
        "\n",
        "eclf = VotingClassifier(estimators=[('lr', DTC), ('rf', RFC),('et',ETC)], voting='hard') \n",
        "for clf, label in zip([DTC, RFC,ETC, eclf], ['DecisionTreeClassifier', 'RandomForestClassifier', 'ExtraTreesClassifier', 'Ensemble']): \n",
        "    _ = clf.fit(X_train, y_train)\n",
        "    pred = clf.score(X_test,y_test)\n",
        "    print(\"Acc: %0.7f [%s]\" % (pred,label))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.8102641 [DecisionTreeClassifier]\n",
            "Acc: 0.8337279 [RandomForestClassifier]\n",
            "Acc: 0.8280385 [ExtraTreesClassifier]\n",
            "Acc: 0.8316477 [Ensemble]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1_fBgr28t9G",
        "outputId": "f088db56-a56c-4dee-9aad-8c7c1afbabe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.decomposition import PCA,TruncatedSVD,PCA\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "rfe = RFE(DecisionTreeClassifier(), 10).fit(X_train,y_train)\n",
        "desiredIndices = np.where(rfe.support_==True)[0]\n",
        "\n",
        "X_train, X_test = pd.DataFrame(X_train),  pd.DataFrame(X_test)\n",
        "\n",
        "whitelist = X_train.columns.values[desiredIndices]\n",
        "X_train_RFE, X_test_RFE = X_train[whitelist],X_test[whitelist]\n",
        "\n",
        "DTC = DecisionTreeClassifier() \n",
        "RFC = RandomForestClassifier(n_estimators=50, random_state=1)\n",
        "ETC = ExtraTreesClassifier(n_estimators=75, criterion='gini', max_features='auto', bootstrap=False)\n",
        "\n",
        "# X_train.shape\n",
        "# X_train_RFE.shape\n",
        "# y_train.shape\n",
        "\n",
        "# print()\n",
        "# X_test.shape\n",
        "# X_test_RFE.shape\n",
        "# y_test.shape\n",
        "\n",
        "eclf = VotingClassifier(estimators=[('lr', DTC), ('rf', RFC),('et',ETC)], voting='hard') \n",
        "for clf, label in zip([DTC, RFC,ETC, eclf], ['DecisionTreeClassifier', 'RandomForestClassifier', 'ExtraTreesClassifier', 'Ensemble']): \n",
        "    _ = clf.fit(X_train_RFE, y_train)\n",
        "    pred = clf.score(X_test_RFE, y_test)\n",
        "    print(\"Acc: %0.7f [%s]\" % (pred,label))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.8058787 [DecisionTreeClassifier]\n",
            "Acc: 0.8269131 [RandomForestClassifier]\n",
            "Acc: 0.8237696 [ExtraTreesClassifier]\n",
            "Acc: 0.8261059 [Ensemble]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFmoE9oW8t9M",
        "outputId": "7f4bf3d2-9b75-4b14-ead3-4dae403e570b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "X_train_RFE.shape\n",
        "set(y_train)\n",
        "\n",
        "# X_train_RFE.dtype = np.int\n",
        "\n",
        "\n",
        "# torch.as_tensor(X_train_RFE_vals[i].astype('double'),dtype=torch.double)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(128836, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWp197rN8t9Q"
      },
      "source": [
        "# Neural network\n",
        "\n",
        "\n",
        "## feed forward\n",
        "\n",
        "*Categorical Variational AutoEncoder with Gumbel-Softmax* at end\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXAYPXsQ9UKN"
      },
      "source": [
        "# RFE on 80% train \n",
        "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=.80, random_state=42) \n",
        "\n",
        "rfe = RFE(DecisionTreeClassifier(), 10).fit(X_train,y_train)\n",
        "desiredIndices = np.where(rfe.support_==True)[0]\n",
        "\n",
        "X_train, X_test = pd.DataFrame(X_train),  pd.DataFrame(X_test)\n",
        "\n",
        "whitelist = X_train.columns.values[desiredIndices]\n",
        "X_train_RFE, X_test_RFE = X_train[whitelist], X_test[whitelist]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqO2js-o9i-9"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = 'cpu'\n",
        "\n",
        "input_size = 10\n",
        "hidden_size = 64 \n",
        "hidden_size_2 = 64\n",
        "num_classes = 10\n",
        "\n",
        "num_epochs = 40\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Fully connected neural network with one hidden layer\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.input_size = input_size # ?? \n",
        "        self.l1 = nn.Linear(input_size, hidden_size) \n",
        "        self.l2 = nn.Linear(hidden_size, hidden_size_2)  \n",
        "        self.l3 = nn.Linear(hidden_size_2, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.elu = nn.ELU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.l1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.l2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.l3(out)\n",
        "        # no activation and no softmax at the end\n",
        "        return out"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTCczWlu8t9R",
        "outputId": "dacb7c46-522a-4bc9-f0f4-23df3efa1549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss() # This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Train the model\n",
        "n_total_steps = len(X_train)\n",
        "\n",
        "X_train_RFE_vals= X_train_RFE.values\n",
        "y_train_vals = y_train.values\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # for i in range(len(X_train_RFE_vals)//100 + 1): #, batch_size\n",
        "    for i in range(0, X_train_RFE_vals.shape[0], batch_size):\n",
        "\n",
        "\n",
        "        x = torch.as_tensor(X_train_RFE_vals[i:i+batch_size], dtype=torch.float).to(device)\n",
        "        y = torch.as_tensor(y_train_vals[i:i+batch_size], dtype=torch.long).to(device)\n",
        "\n",
        "        # x.type()\n",
        "        # y.type()\n",
        "        \n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    if (epoch+1) % 10 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [10/40], Step [51521/51534], Loss: 0.5041\n",
            "Epoch [20/40], Step [51521/51534], Loss: 0.4103\n",
            "Epoch [30/40], Step [51521/51534], Loss: 0.3893\n",
            "Epoch [40/40], Step [51521/51534], Loss: 0.3890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drjRsfmM8t9W",
        "outputId": "1c5f9b3f-dc9d-4f1d-a13f-b8cfe7eaa141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "X_test_RFE_vals= X_test_RFE.values\n",
        "y_test_vals = y_test.values\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0 \n",
        "    # for i in range(len(X_train_RFE_vals)//100 + 1):   \n",
        "    for i in range(0, X_test_RFE_vals.shape[0], batch_size):\n",
        "        x = torch.as_tensor(X_test_RFE_vals[i:i+batch_size], dtype=torch.float).to(device)\n",
        "        y = torch.as_tensor(y_test_vals[i:i+batch_size], dtype=torch.long).to(device)\n",
        "        \n",
        "        outputs = model(x)\n",
        "        if len(outputs.data) > 0:\n",
        "          # max returns (value ,index)\n",
        "          _, predicted = torch.max(outputs.data, dim=1)\n",
        "          n_samples += y.size(0)\n",
        "          n_correct += (predicted == y).sum().item()\n",
        "        else:\n",
        "          print(\"what???\")\n",
        "          print(x, outputs.data)\n",
        "    acc = 100.0 * n_correct / (n_samples+1)\n",
        "    print(f'Accuracy of the network: {acc} %')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network: 78.81439798195402 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-kjC3e78t_l"
      },
      "source": [
        "##  Categorical Variational AutoEncoder with Gumbel-Softmax\n",
        "\n",
        "* https://github.com/YongfeiYan/Gumbel_Softmax_VAE/blob/master/gumbel_softmax_vae.py\n",
        "\n",
        "* https://github.com/shaabhishek/gumbel-softmax-pytorch/blob/master/Categorical%20VAE.ipynb\n",
        "\n",
        "\n",
        "* https://github.com/ericjang/gumbel-softmax/blob/master/Categorical%20VAE.ipynb\n",
        "  * https://blog.evjang.com/2016/11/tutorial-categorical-variational.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFJF8-1cX34F"
      },
      "source": [
        "import copy\n",
        "device = 'cpu' # torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# RFE on 80% train \n",
        "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=.80, random_state=42) \n",
        "\n",
        "rfe = RFE(DecisionTreeClassifier(), 10).fit(X_train,y_train)\n",
        "\n",
        "whitelist = X_train.columns.values[np.where(rfe.support_==True)[0]]\n",
        "X_train_RFE, X_test_RFE = pd.DataFrame(X_train)[whitelist], pd.DataFrame(X_test)[whitelist]\n",
        "\n",
        "X_train = torch.as_tensor(X_train_RFE.to_numpy(), dtype=torch.float).to(device)\n",
        "X_test  = torch.as_tensor(X_test_RFE.to_numpy(), dtype=torch.float).to(device)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDaseCk255M9"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def sample_gumbel(shape, eps=1e-20):\n",
        "    U = torch.rand(shape).to(device) # U is a unif.. waht ever that means.. \n",
        "    # eps = eps.to(device)\n",
        "    return -torch.log(-torch.log(U + eps) + eps)\n",
        "\n",
        "\n",
        "def gumbel_softmax_sample(logits, temperature):\n",
        "    y = logits + sample_gumbel(logits.size())\n",
        "    return F.softmax(y / temperature, dim=-1)\n",
        "\n",
        "\n",
        "def gumbel_softmax(logits, temperature, hard=False):\n",
        "    \"\"\"\n",
        "    ST-gumple-softmax\n",
        "    input: [*, n_class]\n",
        "    return: flatten --> [*, n_class] an one-hot vector\n",
        "    \"\"\"\n",
        "    y = gumbel_softmax_sample(logits, temperature)\n",
        "    \n",
        "    if not hard:\n",
        "        return y.view(-1, latent_dim * categorical_dim)\n",
        "\n",
        "    shape = y.size()\n",
        "    _, ind = y.max(dim=-1)\n",
        "    y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
        "    y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
        "    y_hard = y_hard.view(*shape)\n",
        "    # Set gradients w.r.t. y_hard gradients w.r.t. y\n",
        "    y_hard = (y_hard - y).detach() + y\n",
        "    return y_hard.view(-1, latent_dim * categorical_dim)\n",
        "\n",
        "\n",
        "class VAE_gumbel(nn.Module):\n",
        "    def __init__(self, temp, shape):\n",
        "        super(VAE_gumbel, self).__init__()\n",
        "        print(\"shape:\", shape)\n",
        "\n",
        "        self.batch_size = shape[0]\n",
        "        self.dim = shape[1]\n",
        "        self.mid = 200 \n",
        "        self.inner = 150\n",
        "\n",
        "        self.fc1 = nn.Linear(self.dim * self.batch_size, self.mid)\n",
        "        self.fc2 = nn.Linear(self.mid, self.inner)\n",
        "        self.fc3 = nn.Linear(self.inner , latent_dim * categorical_dim)\n",
        "\n",
        "        self.fc4 = nn.Linear(latent_dim * categorical_dim, self.inner)\n",
        "        self.fc5 = nn.Linear(self.inner, self.mid)\n",
        "        self.fc6 = nn.Linear(self.mid, self.dim * self.batch_size)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = self.relu(self.fc1(x))\n",
        "        h2 = self.relu(self.fc2(h1))\n",
        "        return self.relu(self.fc3(h2))\n",
        "\n",
        "    def decode(self, z):\n",
        "        h4 = self.relu(self.fc4(z))\n",
        "        h5 = self.relu(self.fc5(h4))\n",
        "        return self.sigmoid(self.fc6(h5))\n",
        "\n",
        "    def forward(self, x, temp, hard):\n",
        "        # print('x', x.size())\n",
        "        q = self.encode(x.reshape(-1, self.dim * self.batch_size))\n",
        "        q_y = q.view(q.size(0), latent_dim, categorical_dim)\n",
        "        z = gumbel_softmax(q_y, temp, hard)\n",
        "\n",
        "        # decode(gumbel_softmax(q_y)), softmax(q_y)\n",
        "        return self.decode(z), F.softmax(q_y, dim=-1).reshape(*q.size()) \n",
        "\n",
        "# Reconstruction + KL divergence losses summed over all elements and batch\n",
        "def loss_function(recon_x, x, qy):\n",
        "    # print(recon_x.size()) # torch.Size([1, 640])\n",
        "    # print(x.size())       # torch.Size([64, 10])   # x.size()[0] * x.size()[1]\n",
        "\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.reshape(-1, x.size()[0] * x.size()[1]), size_average=False) / x.shape[0]\n",
        "\n",
        "    log_ratio = torch.log(qy * categorical_dim + 1e-20)\n",
        "    KLD = torch.sum(qy * log_ratio, dim=-1).mean()\n",
        "\n",
        "    return BCE + KLD"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2E897q_zYzY"
      },
      "source": [
        "### Experiment for autoencoder\n",
        "> keep only the features that have a \"huge\" difference between normal and attack  \n",
        "\n",
        "VotingClassifier 83% -> 76% acc\n",
        "\n",
        "AutoEncoder 88.3%-> 82.1%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2dbx04VW5f8",
        "outputId": "e323d49b-b1a7-442e-bf5a-8a98b988c596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "normal = combined_data[combined_data['attack_cat']==6].drop(['attack_cat'], axis=1).apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
        "attack = combined_data[combined_data['attack_cat']!=6].drop(['attack_cat'], axis=1).apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
        "\n",
        "split = int(len(normal)*0.9)\n",
        "\n",
        "normal_train = normal.values[:split]\n",
        "normal_test = normal.values[split:]\n",
        "\n",
        "X_train = torch.as_tensor(normal_train, dtype=torch.float).to(device)\n",
        "X_test = torch.as_tensor(normal_test, dtype=torch.float).to(device)\n",
        "\n",
        "normal_tensor = torch.as_tensor(normal.values, dtype=torch.float).to(device)\n",
        "attack_tensor = torch.as_tensor(attack.values, dtype=torch.float).to(device)\n",
        "\n",
        "print(\"attack_tensor.shape: \", attack.values.shape)\n",
        "print(\"normal.shape: \", normal.values.shape)\n",
        "print(\"X_train.shape: \", X_train.size())\n",
        "print(\"X_test.shape: \", X_test.size())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attack_tensor.shape:  (164673, 31)\n",
            "normal.shape:  (93000, 31)\n",
            "X_train.shape:  torch.Size([83700, 31])\n",
            "X_test.shape:  torch.Size([9300, 31])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3F0RW_hDoRm",
        "outputId": "b049786f-47ea-4a1f-cc97-2924f5e102d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train(epoch, temp=1, hard=False):\n",
        "    model.train()\n",
        "    # train_loss = 0\n",
        "\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        if (i+batch_size > X_train.shape[0]):\n",
        "            break # avoid the issue of odd batch size...\n",
        "        data = X_train[i:i+batch_size] #.to(device)\n",
        "\n",
        "        # decode(gumbel_softmax(q_y)), softmax(q_y)\n",
        "        recon_batch, qy = model(data, temp, hard)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_function(recon_batch, data, qy)\n",
        "        loss.backward()\n",
        "        # train_loss += loss.item() * len(data)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if i % batch_size == 1:\n",
        "          temp = np.maximum(temp * np.exp(-ANNEAL_RATE * i), temp_min)\n",
        "        \n",
        "        if i % 3000 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, i * len(data), len(X_train),\n",
        "                       100. * i / len(X_train),\n",
        "                       loss.item()))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "        epoch, loss.item() ))\n",
        "\n",
        "\n",
        "def test(epoch, best_loss, temp=1, hard=False):\n",
        "    model.eval()\n",
        "    test_loss = []\n",
        "\n",
        "    for i in range(0, X_test.shape[0], batch_size):\n",
        "        if (i+batch_size > X_test.shape[0]):\n",
        "            break # avoid the issue of odd batch size...\n",
        "        data = X_test[i:i+batch_size] # .to(device)\n",
        "        recon_batch, qy = model(data, temp, hard)\n",
        "        _loss = loss_function(recon_batch, data, qy).item()\n",
        "        test_loss.append(_loss)\n",
        "        if i % 100 == 1:\n",
        "          temp = np.maximum(temp * np.exp(-ANNEAL_RATE * i), temp_min)\n",
        "\n",
        "    val_loss = sum(test_loss)/len(test_loss)\n",
        "    print(f'====> Test set loss: {val_loss:.4f}')\n",
        "    scheduler.step(val_loss) # ReduceLROnPlateau\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "      best_loss = val_loss\n",
        "      best_model_wts = copy.deepcopy(model.state_dict())\n",
        "      print(f\"Best loss is {val_loss}, weights saved*\")\n",
        "    return best_loss\n",
        "\n",
        "latent_dim = 10 # was 30 \n",
        "categorical_dim = 10 # was 10  # one-of-K vector\n",
        "\n",
        "temp = 1\n",
        "temp_min = 0.5\n",
        "ANNEAL_RATE = 0.00003\n",
        "\n",
        "epochs = 16\n",
        "batch_size = 36\n",
        "\n",
        "model = VAE_gumbel(temp, (batch_size, X_train.shape[1])) # default=1.0, metavar='S', help='tau(temperature) (default: 1.0)')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0009) # 1e-3\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
        "\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_loss = 100000000000.0 \n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch)\n",
        "    best_loss = test(epoch, best_loss)\n",
        "\n",
        "    M = 64 * latent_dim\n",
        "    np_y = np.zeros((M, categorical_dim), dtype=np.float32)\n",
        "    np_y[range(M), np.random.choice(categorical_dim, M)] = 1\n",
        "    np_y = np.reshape(np_y, [M // latent_dim, latent_dim, categorical_dim])\n",
        "    sample = torch.from_numpy(np_y).view(M // latent_dim, latent_dim * categorical_dim)\n",
        "    sample = sample.to(device)\n",
        "    sample = model.decode(sample).cpu()\n",
        "    print(sample.size())\n",
        "\n",
        "model.load_state_dict(best_model_wts)\n",
        "# torch.save(model, 'model.pth')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape: (36, 31)\n",
            "Train Epoch: 1 [0/83700 (0%)]\tLoss: 21.525465\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [324000/83700 (11%)]\tLoss: 7.878410\n",
            "Train Epoch: 1 [648000/83700 (22%)]\tLoss: 8.001693\n",
            "Train Epoch: 1 [972000/83700 (32%)]\tLoss: 7.939018\n",
            "Train Epoch: 1 [1296000/83700 (43%)]\tLoss: 8.209831\n",
            "Train Epoch: 1 [1620000/83700 (54%)]\tLoss: 8.280560\n",
            "Train Epoch: 1 [1944000/83700 (65%)]\tLoss: 11.108716\n",
            "Train Epoch: 1 [2268000/83700 (75%)]\tLoss: 8.660810\n",
            "Train Epoch: 1 [2592000/83700 (86%)]\tLoss: 8.643674\n",
            "Train Epoch: 1 [2916000/83700 (97%)]\tLoss: 8.244834\n",
            "====> Epoch: 1 Average loss: 8.1323\n",
            "====> Test set loss: 10.8159\n",
            "Best loss is 10.815860598586326, weights saved*\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 2 [0/83700 (0%)]\tLoss: 10.578753\n",
            "Train Epoch: 2 [324000/83700 (11%)]\tLoss: 7.879595\n",
            "Train Epoch: 2 [648000/83700 (22%)]\tLoss: 8.005692\n",
            "Train Epoch: 2 [972000/83700 (32%)]\tLoss: 7.939285\n",
            "Train Epoch: 2 [1296000/83700 (43%)]\tLoss: 8.199089\n",
            "Train Epoch: 2 [1620000/83700 (54%)]\tLoss: 8.288415\n",
            "Train Epoch: 2 [1944000/83700 (65%)]\tLoss: 11.254662\n",
            "Train Epoch: 2 [2268000/83700 (75%)]\tLoss: 8.572467\n",
            "Train Epoch: 2 [2592000/83700 (86%)]\tLoss: 8.459499\n",
            "Train Epoch: 2 [2916000/83700 (97%)]\tLoss: 8.212085\n",
            "====> Epoch: 2 Average loss: 8.0667\n",
            "====> Test set loss: 10.6958\n",
            "Best loss is 10.695833276408587, weights saved*\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 3 [0/83700 (0%)]\tLoss: 10.678325\n",
            "Train Epoch: 3 [324000/83700 (11%)]\tLoss: 7.883219\n",
            "Train Epoch: 3 [648000/83700 (22%)]\tLoss: 7.992506\n",
            "Train Epoch: 3 [972000/83700 (32%)]\tLoss: 7.937459\n",
            "Train Epoch: 3 [1296000/83700 (43%)]\tLoss: 8.183577\n",
            "Train Epoch: 3 [1620000/83700 (54%)]\tLoss: 8.285413\n",
            "Train Epoch: 3 [1944000/83700 (65%)]\tLoss: 11.313686\n",
            "Train Epoch: 3 [2268000/83700 (75%)]\tLoss: 8.591683\n",
            "Train Epoch: 3 [2592000/83700 (86%)]\tLoss: 8.472843\n",
            "Train Epoch: 3 [2916000/83700 (97%)]\tLoss: 8.158664\n",
            "====> Epoch: 3 Average loss: 8.0946\n",
            "====> Test set loss: 10.5136\n",
            "Best loss is 10.513614331104959, weights saved*\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 4 [0/83700 (0%)]\tLoss: 10.409226\n",
            "Train Epoch: 4 [324000/83700 (11%)]\tLoss: 7.865052\n",
            "Train Epoch: 4 [648000/83700 (22%)]\tLoss: 7.984756\n",
            "Train Epoch: 4 [972000/83700 (32%)]\tLoss: 7.935231\n",
            "Train Epoch: 4 [1296000/83700 (43%)]\tLoss: 8.177025\n",
            "Train Epoch: 4 [1620000/83700 (54%)]\tLoss: 8.288540\n",
            "Train Epoch: 4 [1944000/83700 (65%)]\tLoss: 11.369428\n",
            "Train Epoch: 4 [2268000/83700 (75%)]\tLoss: 8.678776\n",
            "Train Epoch: 4 [2592000/83700 (86%)]\tLoss: 8.375156\n",
            "Train Epoch: 4 [2916000/83700 (97%)]\tLoss: 8.143303\n",
            "====> Epoch: 4 Average loss: 8.0639\n",
            "====> Test set loss: 10.4277\n",
            "Best loss is 10.42766176637753, weights saved*\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 5 [0/83700 (0%)]\tLoss: 10.495441\n",
            "Train Epoch: 5 [324000/83700 (11%)]\tLoss: 7.873394\n",
            "Train Epoch: 5 [648000/83700 (22%)]\tLoss: 7.986607\n",
            "Train Epoch: 5 [972000/83700 (32%)]\tLoss: 7.930926\n",
            "Train Epoch: 5 [1296000/83700 (43%)]\tLoss: 8.175957\n",
            "Train Epoch: 5 [1620000/83700 (54%)]\tLoss: 8.288134\n",
            "Train Epoch: 5 [1944000/83700 (65%)]\tLoss: 11.424516\n",
            "Train Epoch: 5 [2268000/83700 (75%)]\tLoss: 8.628638\n",
            "Train Epoch: 5 [2592000/83700 (86%)]\tLoss: 8.391949\n",
            "Train Epoch: 5 [2916000/83700 (97%)]\tLoss: 8.147177\n",
            "====> Epoch: 5 Average loss: 8.0518\n",
            "====> Test set loss: 10.4141\n",
            "Best loss is 10.414058725963267, weights saved*\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 6 [0/83700 (0%)]\tLoss: 10.461349\n",
            "Train Epoch: 6 [324000/83700 (11%)]\tLoss: 7.895933\n",
            "Train Epoch: 6 [648000/83700 (22%)]\tLoss: 7.977252\n",
            "Train Epoch: 6 [972000/83700 (32%)]\tLoss: 7.938057\n",
            "Train Epoch: 6 [1296000/83700 (43%)]\tLoss: 8.158548\n",
            "Train Epoch: 6 [1620000/83700 (54%)]\tLoss: 8.289036\n",
            "Train Epoch: 6 [1944000/83700 (65%)]\tLoss: 11.452525\n",
            "Train Epoch: 6 [2268000/83700 (75%)]\tLoss: 8.568784\n",
            "Train Epoch: 6 [2592000/83700 (86%)]\tLoss: 8.378579\n",
            "Train Epoch: 6 [2916000/83700 (97%)]\tLoss: 8.111961\n",
            "====> Epoch: 6 Average loss: 8.1348\n",
            "====> Test set loss: 10.3709\n",
            "Best loss is 10.370913490768551, weights saved*\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 7 [0/83700 (0%)]\tLoss: 10.371182\n",
            "Train Epoch: 7 [324000/83700 (11%)]\tLoss: 7.866140\n",
            "Train Epoch: 7 [648000/83700 (22%)]\tLoss: 7.973528\n",
            "Train Epoch: 7 [972000/83700 (32%)]\tLoss: 7.927772\n",
            "Train Epoch: 7 [1296000/83700 (43%)]\tLoss: 8.163932\n",
            "Train Epoch: 7 [1620000/83700 (54%)]\tLoss: 8.291242\n",
            "Train Epoch: 7 [1944000/83700 (65%)]\tLoss: 11.458041\n",
            "Train Epoch: 7 [2268000/83700 (75%)]\tLoss: 8.583531\n",
            "Train Epoch: 7 [2592000/83700 (86%)]\tLoss: 8.202033\n",
            "Train Epoch: 7 [2916000/83700 (97%)]\tLoss: 8.140812\n",
            "====> Epoch: 7 Average loss: 8.0257\n",
            "====> Test set loss: 10.3828\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 8 [0/83700 (0%)]\tLoss: 10.448368\n",
            "Train Epoch: 8 [324000/83700 (11%)]\tLoss: 7.861212\n",
            "Train Epoch: 8 [648000/83700 (22%)]\tLoss: 7.963334\n",
            "Train Epoch: 8 [972000/83700 (32%)]\tLoss: 7.925222\n",
            "Train Epoch: 8 [1296000/83700 (43%)]\tLoss: 8.158175\n",
            "Train Epoch: 8 [1620000/83700 (54%)]\tLoss: 8.292193\n",
            "Train Epoch: 8 [1944000/83700 (65%)]\tLoss: 11.410572\n",
            "Train Epoch: 8 [2268000/83700 (75%)]\tLoss: 8.588517\n",
            "Train Epoch: 8 [2592000/83700 (86%)]\tLoss: 8.236976\n",
            "Train Epoch: 8 [2916000/83700 (97%)]\tLoss: 8.113283\n",
            "====> Epoch: 8 Average loss: 8.0374\n",
            "====> Test set loss: 10.3421\n",
            "Best loss is 10.342141023902006, weights saved*\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 9 [0/83700 (0%)]\tLoss: 10.496258\n",
            "Train Epoch: 9 [324000/83700 (11%)]\tLoss: 7.874058\n",
            "Train Epoch: 9 [648000/83700 (22%)]\tLoss: 7.959821\n",
            "Train Epoch: 9 [972000/83700 (32%)]\tLoss: 7.922328\n",
            "Train Epoch: 9 [1296000/83700 (43%)]\tLoss: 8.154346\n",
            "Train Epoch: 9 [1620000/83700 (54%)]\tLoss: 8.291295\n",
            "Train Epoch: 9 [1944000/83700 (65%)]\tLoss: 11.573787\n",
            "Train Epoch: 9 [2268000/83700 (75%)]\tLoss: 8.594179\n",
            "Train Epoch: 9 [2592000/83700 (86%)]\tLoss: 8.245466\n",
            "Train Epoch: 9 [2916000/83700 (97%)]\tLoss: 8.129589\n",
            "====> Epoch: 9 Average loss: 8.0466\n",
            "====> Test set loss: 10.3238\n",
            "Best loss is 10.323849313942961, weights saved*\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 10 [0/83700 (0%)]\tLoss: 10.424709\n",
            "Train Epoch: 10 [324000/83700 (11%)]\tLoss: 7.867360\n",
            "Train Epoch: 10 [648000/83700 (22%)]\tLoss: 7.956824\n",
            "Train Epoch: 10 [972000/83700 (32%)]\tLoss: 7.922538\n",
            "Train Epoch: 10 [1296000/83700 (43%)]\tLoss: 8.157007\n",
            "Train Epoch: 10 [1620000/83700 (54%)]\tLoss: 8.293655\n",
            "Train Epoch: 10 [1944000/83700 (65%)]\tLoss: 11.591105\n",
            "Train Epoch: 10 [2268000/83700 (75%)]\tLoss: 8.596237\n",
            "Train Epoch: 10 [2592000/83700 (86%)]\tLoss: 8.183311\n",
            "Train Epoch: 10 [2916000/83700 (97%)]\tLoss: 8.111519\n",
            "====> Epoch: 10 Average loss: 8.0421\n",
            "====> Test set loss: 10.3400\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 11 [0/83700 (0%)]\tLoss: 10.449116\n",
            "Train Epoch: 11 [324000/83700 (11%)]\tLoss: 7.872747\n",
            "Train Epoch: 11 [648000/83700 (22%)]\tLoss: 7.960109\n",
            "Train Epoch: 11 [972000/83700 (32%)]\tLoss: 7.929423\n",
            "Train Epoch: 11 [1296000/83700 (43%)]\tLoss: 8.156220\n",
            "Train Epoch: 11 [1620000/83700 (54%)]\tLoss: 8.290670\n",
            "Train Epoch: 11 [1944000/83700 (65%)]\tLoss: 11.574900\n",
            "Train Epoch: 11 [2268000/83700 (75%)]\tLoss: 8.589529\n",
            "Train Epoch: 11 [2592000/83700 (86%)]\tLoss: 8.240067\n",
            "Train Epoch: 11 [2916000/83700 (97%)]\tLoss: 8.130378\n",
            "====> Epoch: 11 Average loss: 8.0559\n",
            "====> Test set loss: 10.3571\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 12 [0/83700 (0%)]\tLoss: 10.482214\n",
            "Train Epoch: 12 [324000/83700 (11%)]\tLoss: 7.869216\n",
            "Train Epoch: 12 [648000/83700 (22%)]\tLoss: 7.963016\n",
            "Train Epoch: 12 [972000/83700 (32%)]\tLoss: 7.922278\n",
            "Train Epoch: 12 [1296000/83700 (43%)]\tLoss: 8.154634\n",
            "Train Epoch: 12 [1620000/83700 (54%)]\tLoss: 8.293502\n",
            "Train Epoch: 12 [1944000/83700 (65%)]\tLoss: 11.639944\n",
            "Train Epoch: 12 [2268000/83700 (75%)]\tLoss: 8.611790\n",
            "Train Epoch: 12 [2592000/83700 (86%)]\tLoss: 8.294271\n",
            "Train Epoch: 12 [2916000/83700 (97%)]\tLoss: 8.145432\n",
            "====> Epoch: 12 Average loss: 8.0049\n",
            "====> Test set loss: 10.3596\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 13 [0/83700 (0%)]\tLoss: 10.493465\n",
            "Train Epoch: 13 [324000/83700 (11%)]\tLoss: 7.867559\n",
            "Train Epoch: 13 [648000/83700 (22%)]\tLoss: 7.956903\n",
            "Train Epoch: 13 [972000/83700 (32%)]\tLoss: 7.921932\n",
            "Train Epoch: 13 [1296000/83700 (43%)]\tLoss: 8.150105\n",
            "Train Epoch: 13 [1620000/83700 (54%)]\tLoss: 8.291972\n",
            "Train Epoch: 13 [1944000/83700 (65%)]\tLoss: 11.628121\n",
            "Train Epoch: 13 [2268000/83700 (75%)]\tLoss: 8.580513\n",
            "Train Epoch: 13 [2592000/83700 (86%)]\tLoss: 8.200084\n",
            "Train Epoch: 13 [2916000/83700 (97%)]\tLoss: 8.133934\n",
            "====> Epoch: 13 Average loss: 8.0472\n",
            "====> Test set loss: 10.3473\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 14 [0/83700 (0%)]\tLoss: 10.461828\n",
            "Train Epoch: 14 [324000/83700 (11%)]\tLoss: 9.319778\n",
            "Train Epoch: 14 [648000/83700 (22%)]\tLoss: 8.724461\n",
            "Train Epoch: 14 [972000/83700 (32%)]\tLoss: 8.310722\n",
            "Train Epoch: 14 [1296000/83700 (43%)]\tLoss: 8.316400\n",
            "Train Epoch: 14 [1620000/83700 (54%)]\tLoss: 8.438298\n",
            "Train Epoch: 14 [1944000/83700 (65%)]\tLoss: 11.700234\n",
            "Train Epoch: 14 [2268000/83700 (75%)]\tLoss: 10.154122\n",
            "Train Epoch: 14 [2592000/83700 (86%)]\tLoss: 9.683242\n",
            "Train Epoch: 14 [2916000/83700 (97%)]\tLoss: 8.494244\n",
            "====> Epoch: 14 Average loss: 8.4631\n",
            "====> Test set loss: 10.2018\n",
            "Best loss is 10.20180357149405, weights saved*\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 15 [0/83700 (0%)]\tLoss: 10.537369\n",
            "Train Epoch: 15 [324000/83700 (11%)]\tLoss: 8.518907\n",
            "Train Epoch: 15 [648000/83700 (22%)]\tLoss: 8.359122\n",
            "Train Epoch: 15 [972000/83700 (32%)]\tLoss: 8.125217\n",
            "Train Epoch: 15 [1296000/83700 (43%)]\tLoss: 8.269383\n",
            "Train Epoch: 15 [1620000/83700 (54%)]\tLoss: 8.390240\n",
            "Train Epoch: 15 [1944000/83700 (65%)]\tLoss: 11.725438\n",
            "Train Epoch: 15 [2268000/83700 (75%)]\tLoss: 10.293331\n",
            "Train Epoch: 15 [2592000/83700 (86%)]\tLoss: 9.244443\n",
            "Train Epoch: 15 [2916000/83700 (97%)]\tLoss: 8.503283\n",
            "====> Epoch: 15 Average loss: 8.4464\n",
            "====> Test set loss: 10.2082\n",
            "torch.Size([64, 1116])\n",
            "Train Epoch: 16 [0/83700 (0%)]\tLoss: 10.581737\n",
            "Train Epoch: 16 [324000/83700 (11%)]\tLoss: 8.436717\n",
            "Train Epoch: 16 [648000/83700 (22%)]\tLoss: 8.444159\n",
            "Train Epoch: 16 [972000/83700 (32%)]\tLoss: 8.131420\n",
            "Train Epoch: 16 [1296000/83700 (43%)]\tLoss: 8.267922\n",
            "Train Epoch: 16 [1620000/83700 (54%)]\tLoss: 8.343828\n",
            "Train Epoch: 16 [1944000/83700 (65%)]\tLoss: 11.768726\n",
            "Train Epoch: 16 [2268000/83700 (75%)]\tLoss: 10.128297\n",
            "Train Epoch: 16 [2592000/83700 (86%)]\tLoss: 9.384079\n",
            "Train Epoch: 16 [2916000/83700 (97%)]\tLoss: 8.257194\n",
            "====> Epoch: 16 Average loss: 8.4650\n",
            "====> Test set loss: 10.2092\n",
            "torch.Size([64, 1116])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdmQFImLW5Yf"
      },
      "source": [
        "def predict(dataset, temp=1, hard=False):\n",
        "    model.eval()\n",
        "    test_loss = []\n",
        "\n",
        "    # for i, (data, _) in enumerate(test_loader):\n",
        "    for i in range(0, dataset.shape[0], batch_size):\n",
        "        if (i+batch_size > dataset.shape[0]):\n",
        "            break # avoid the issue of odd batch size...\n",
        "            \n",
        "        data = dataset[i:i+batch_size] # .to(device)\n",
        "        recon_batch, qy = model(data, temp, hard)\n",
        "        test_loss.append(loss_function(recon_batch, data, qy).item() * len(data))\n",
        "        # if i % 100 == 1:\n",
        "        temp = np.maximum(temp * np.exp(-ANNEAL_RATE * i), temp_min)\n",
        "\n",
        "    print(f'====> Dataset set loss: {sum(test_loss)/len(test_loss):.4f}')\n",
        "    return test_loss"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLBPGtGDW5Wg",
        "outputId": "7b05fef9-b330-4cab-92ce-05fdd36b52db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "print(X_train.size())\n",
        "losses_train  = predict(X_train)\n",
        "losses_test   = predict(X_test)\n",
        "losses_attack = predict(attack_tensor)\n",
        "losses_normal = predict(normal_tensor)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([83700, 31])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "====> Dataset set loss: 774.5191\n",
            "====> Dataset set loss: 774.9292\n",
            "====> Dataset set loss: 774.9846\n",
            "====> Dataset set loss: 774.5649\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyftzVPjW5SM",
        "outputId": "3358ff51-1c2a-49c1-9c33-1e1ab206fe48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "import seaborn as sns\n",
        "# sns.distplot(losses, bins=50, kde=False);\n",
        "# sns.kdeplot(losses);\n",
        "# plt.hist(losses_train, bins=50);\n",
        "# plt.hist(losses_test, bins=50);\n",
        "# plt.hist(losses_attack, bins=50);\n",
        "# plt.hist(losses_normal, bins=50);\n",
        "\n",
        "sns.kdeplot(losses_normal, color='b');\n",
        "sns.kdeplot(losses_attack, color='r');\n",
        "# sns.distplot(losses_normal, bins=50, kde=True);\n",
        "# sns.distplot(losses_attack, bins=50, kde=True);"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WknzqVLAtmcL"
      },
      "source": [
        "## NOTE\n",
        "\n",
        "SET `lower` & `upper` to an eyeball approximationof the **BLUE** peak\n",
        "> I'm my other notebook, this with with the red peak"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zlz6YORRlXf",
        "outputId": "1aaf98a4-65ff-4529-8472-63d2960d3789",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Brute force compute cutoffs that maximize accuracy  \n",
        "lower = 660 # select the red peak\n",
        "upper = 774.75\n",
        "\n",
        "# lower_list = [lower + round(-7+(0.5*i), 2) for i in range(31)] # [lower-1,lower-0.5,lower,lower+0.5,lower+1]\n",
        "# upper_list = [upper + round(-7+(0.5*i), 2) for i in range(31)] # [upper-1,upper-0.5,upper,upper+0.5,upper+1]\n",
        "lower_list = [lower + round(-1+(0.05*i), 2) for i in range(31)] # [lower-1,lower-0.5,lower,lower+0.5,lower+1]\n",
        "upper_list = [upper + round(-1+(0.05*i), 2) for i in range(31)] # [upper-1,upper-0.5,upper,upper+0.5,upper+1]\n",
        "ls_1 = []\n",
        "ls_2 = []\n",
        "print('lower_list', lower_list[0], 'to', lower_list[-1])\n",
        "\n",
        "for i in range(len(lower_list)):\n",
        "  lower = lower_list[i]\n",
        "  for i in range(len(upper_list)):\n",
        "    upper = upper_list[i]\n",
        "\n",
        "    all_attack = np.array(losses_attack)\n",
        "    FP = all_attack[(all_attack > lower) & (all_attack < upper)]\n",
        "    TN = all_attack[(all_attack <= lower) | (all_attack >= upper)]\n",
        "\n",
        "    all_normal = np.array(losses_normal)\n",
        "    TP = all_normal[(all_normal > lower) & (all_normal < upper)]\n",
        "    FN = all_normal[(all_normal <= lower) | (all_normal >= upper)]\n",
        "\n",
        "    ls_1.append((len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN)))\n",
        "    ls_2.append((lower, upper))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lower_list 659.0 to 660.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP47kaFZRszs",
        "outputId": "84cbde92-6830-4505-b2ce-39f6fc548d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "idx = np.argmax(np.array(ls_1))\n",
        "# print(ls_1[idx])\n",
        "# print(ls_2[idx])\n",
        "lower = ls_2[idx][0]\n",
        "upper = ls_2[idx][1]\n",
        "\n",
        "all_attack = np.array(losses_attack)\n",
        "FP = all_attack[(all_attack > lower) & (all_attack < upper)]\n",
        "TN = all_attack[(all_attack <= lower) | (all_attack >= upper)]\n",
        "\n",
        "all_normal = np.array(losses_normal)\n",
        "TP = all_normal[(all_normal > lower) & (all_normal < upper)]\n",
        "FN = all_normal[(all_normal <= lower) | (all_normal >= upper)]\n",
        "\n",
        "ls_1.append((len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN)))\n",
        "ls_2.append((lower, upper))\n",
        "\n",
        "print(f'TP: {len(TP)}')\n",
        "print(f'FN: {len(FN)}')\n",
        "print(f'TPR: {len(TP)/(len(TP)+len(FN))}')\n",
        "\n",
        "print(f'TN: {len(TN)}')\n",
        "print(f'FP: {len(FP)}')\n",
        "print(f'TNR: {len(TN)/(len(TN)+len(FP))}')\n",
        "\n",
        "print(f'Accuracy : {(len(TP)+len(TN))/(len(TP)+len(TN)+len(FP)+len(FN))}')\n",
        "# https://en.wikipedia.org/wiki/Precision_and_recall#Imbalanced_data"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TP: 1104\n",
            "FN: 1479\n",
            "TPR: 0.4274099883855981\n",
            "TN: 4263\n",
            "FP: 311\n",
            "TNR: 0.9320069960647136\n",
            "Accuracy : 0.7498952074891715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veFmAWFOR-dp"
      },
      "source": [
        "## Best is 81% Acc"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F92eikGSHkW"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R6n6VMPSI3-"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}