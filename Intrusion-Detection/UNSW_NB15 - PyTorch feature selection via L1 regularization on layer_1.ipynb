{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/alik604/cyber-security/blob/master/Intrusion-Detection/UNSW_NB15%20-%20PyTorch%20feature%20selection%20via%20L1%20regularization%20on%20layer_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl-cPhNxn6QU"
   },
   "source": [
    " ## References \n",
    "\n",
    " *note that this is binary classification*\n",
    " \n",
    " MLP with pytorch at end\n",
    " \n",
    " * Data source: https://www.unsw.adfa.edu.au/unsw-canberra-cyber/cybersecurity/ADFA-NB15-Datasets/\n",
    " * Sample/starter code: https://github.com/Nir-J/ML-Projects/blob/master/UNSW-Network_Packet_Classification/unsw.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Lx1Zh6UUU-ov"
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as matplot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.metrics import *\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder,normalize\n",
    "\n",
    "\n",
    "import xgboost, lightgbm\n",
    "from mlxtend.classifier import EnsembleVoteClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2Kvca3apvAc"
   },
   "source": [
    "# Preprocessing (transformation/scaling) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vjw9JrARVJ23"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('https://raw.githubusercontent.com/Nir-J/ML-Projects/master/UNSW-Network_Packet_Classification/UNSW_NB15_training-set.csv')\n",
    "test = pd.read_csv('https://raw.githubusercontent.com/Nir-J/ML-Projects/master/UNSW-Network_Packet_Classification/UNSW_NB15_testing-set.csv')\n",
    "combined_data = pd.concat([train, test]).drop(['id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "C5hh-CdxbjCZ",
    "outputId": "ea3347a3-bb7f-488d-9afb-951c5df4eb41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train contamination  0.68\n",
      "test  contamination  0.55 \n",
      "\n",
      "contamination is 0.68, which is greater than 0.5. Fixing...\n",
      "contamination is now 0.32\n"
     ]
    }
   ],
   "source": [
    "# Contaminsation mean pollution (outliers) in data\n",
    "tmp = train.where(train['attack_cat'] == \"Normal\").dropna()\n",
    "contamination = round(1 - len(tmp)/len(train), 2)\n",
    "print(\"train contamination \", contamination)\n",
    "\n",
    "tmp = test.where(test['attack_cat'] == \"Normal\").dropna()\n",
    "print(\"test  contamination \", round(1 - len(tmp)/len(test),2),'\\n')\n",
    "\n",
    "if contamination > 0.5:\n",
    "    print(f'contamination is {contamination}, which is greater than 0.5. Fixing...')\n",
    "    contamination = round(1-contamination,2)\n",
    "    print(f'contamination is now {contamination}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "id": "_tfrFURcVJ6X",
    "outputId": "bfe09d3e-fb08-47e6-9ee1-0d4f2703df35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attack cat: {'Generic', 'Normal', 'Exploits', 'Analysis', 'Worms', 'Reconnaissance', 'Fuzzers', 'Backdoor', 'Shellcode', 'DoS'}\n",
      "\n",
      "Describing attack_type: \n",
      "min 0\n",
      "max 9\n",
      "mode 0    6\n",
      "dtype: int32 Which is, ['Normal']\n",
      "mode 0.3609225646458884 %\n"
     ]
    }
   ],
   "source": [
    "le1 = LabelEncoder()\n",
    "le = LabelEncoder()\n",
    "\n",
    "vector = combined_data['attack_cat']\n",
    "\n",
    "print(\"attack cat:\", set(list(vector))) # use print to make it print on single line \n",
    "\n",
    "combined_data['attack_cat'] = le1.fit_transform(vector)\n",
    "combined_data['proto'] = le.fit_transform(combined_data['proto'])\n",
    "combined_data['service'] = le.fit_transform(combined_data['service'])\n",
    "combined_data['state'] = le.fit_transform(combined_data['state'])\n",
    "\n",
    "vector = combined_data['attack_cat']\n",
    "print('\\nDescribing attack_type: ')\n",
    "print(\"min\", vector.min())\n",
    "print(\"max\", vector.max())\n",
    "print(\"mode\",vector.mode(), \"Which is,\", le1.inverse_transform(vector.mode()))\n",
    "print(\"mode\", len(np.where(vector.values==6)[0])/len(vector),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "FdGigIypbQfd",
    "outputId": "12c71b0c-a8e2-426a-a154-8a04c35ff867"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Analysis', 'Backdoor', 'DoS', 'Exploits', 'Fuzzers', 'Generic',\n",
       "       'Normal', 'Reconnaissance', 'Shellcode', 'Worms'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dur</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>state</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>sttl</th>\n",
       "      <th>...</th>\n",
       "      <th>ct_dst_sport_ltm</th>\n",
       "      <th>ct_dst_src_ltm</th>\n",
       "      <th>is_ftp_login</th>\n",
       "      <th>ct_ftp_cmd</th>\n",
       "      <th>ct_flw_http_mthd</th>\n",
       "      <th>ct_src_ltm</th>\n",
       "      <th>ct_srv_dst</th>\n",
       "      <th>is_sm_ips_ports</th>\n",
       "      <th>attack_cat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.121478</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>258</td>\n",
       "      <td>172</td>\n",
       "      <td>74.087490</td>\n",
       "      <td>252</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.649902</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>734</td>\n",
       "      <td>42014</td>\n",
       "      <td>78.473372</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.623129</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>364</td>\n",
       "      <td>13186</td>\n",
       "      <td>14.170161</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dur  proto  service  state  spkts  dpkts  sbytes  dbytes       rate  \\\n",
       "0  0.121478    113        0      4      6      4     258     172  74.087490   \n",
       "1  0.649902    113        0      4     14     38     734   42014  78.473372   \n",
       "2  1.623129    113        0      4      8     16     364   13186  14.170161   \n",
       "\n",
       "   sttl  ...  ct_dst_sport_ltm  ct_dst_src_ltm  is_ftp_login  ct_ftp_cmd  \\\n",
       "0   252  ...                 1               1             0           0   \n",
       "1    62  ...                 1               2             0           0   \n",
       "2    62  ...                 1               3             0           0   \n",
       "\n",
       "   ct_flw_http_mthd  ct_src_ltm  ct_srv_dst  is_sm_ips_ports  attack_cat  \\\n",
       "0                 0           1           1                0           6   \n",
       "1                 0           1           6                0           6   \n",
       "2                 0           2           6                0           6   \n",
       "\n",
       "   label  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "\n",
       "[3 rows x 44 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le1.inverse_transform([0,1,2,3,4,5,6,7,8,9])\n",
    "combined_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "1bFqsTemVJ92"
   },
   "outputs": [],
   "source": [
    "## OMITTED: For statistical feature removal\n",
    "\n",
    "lowSTD = list(combined_data.std().to_frame().nsmallest(6, columns=0).index)\n",
    "# this is stupid. suppose a feature has a 1.0 (spearman or pearson) correlation, OR conditional probability, when not 0.... That a very useful feature  \n",
    "\n",
    "lowCORR = list(combined_data.corr().abs().sort_values('attack_cat')['attack_cat'].nsmallest(3).index) # .where(lambda x: x < 0.005).dropna()\n",
    "# This might be stupid. A Deep MLP (feed forward neural net) may see patterns\n",
    "\n",
    "drop = set( lowCORR + lowSTD)\n",
    "drop = {'ackdat', 'ct_ftp_cmd', 'djit', 'is_ftp_login', 'is_sm_ips_ports', 'response_body_len', 'sjit', 'synack', 'tcprtt'}\n",
    "# print(f'Before {combined_data.shape}')\n",
    "combined_data_reduced=combined_data # .drop(drop,axis=1)\n",
    "# print(f'After {combined_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Pjuk4i-aVAx7"
   },
   "outputs": [],
   "source": [
    "# # transform = list(combined_data_reduced.columns.values[4:])\n",
    "# transform.append('dur')\n",
    "# transform.remove('attack_cat')\n",
    "# # transform min-max norm \n",
    "# combined_data_reduced[transform] = combined_data_reduced[transform].apply(lambda x: (x - x.min()) / (x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uRDvu_a8VA1q"
   },
   "outputs": [],
   "source": [
    "data_x = combined_data_reduced.drop(['attack_cat','label'], axis=1) # droped label\n",
    "data_y = combined_data_reduced.loc[:,['label']]\n",
    "# del combined_data # free mem\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x, data_y, test_size=.20, random_state=42) # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "l53VPdZ0ZA95"
   },
   "outputs": [],
   "source": [
    "#combined_data_reduced.where(combined_data_reduced['label'] == 1.0).dropna().tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "xjNm11s-fqvw",
    "outputId": "6d5a85e7-eb40-4e52-ede4-9506f34cced4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(206138, 42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(206138, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(51535, 42)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(51535, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "y_train.shape\n",
    "X_test.shape # test is larger... good \n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVO_4gmThrU-"
   },
   "source": [
    "# Benchmark before feature removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "YZYIJ9d_VA57",
    "outputId": "6628ea42-e399-4de9-f510-197b67cbdae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.93752 for the DecisionTreeClassifier\n",
      "Acc: 0.95240 for the RandomForestClassifier\n",
      "Acc: 0.95133 for the ExtraTreesClassifier\n",
      "Acc: 0.93653 for the XGBClassifier\n",
      "Acc: 0.95269 for the LGBMClassifier\n"
     ]
    }
   ],
   "source": [
    "DTC = DecisionTreeClassifier()\n",
    "RFC = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=-1)\n",
    "ETC = ExtraTreesClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "XGB = xgboost.XGBClassifier(n_estimators=150, n_jobs=-1)\n",
    "GBM = lightgbm.LGBMClassifier(objective='binary', n_estimators= 500) # multiclass\n",
    "\n",
    "list_of_CLFs_names = []\n",
    "list_of_CLFs = [DTC, RFC, ETC, XGB, GBM]\n",
    "ranking = []\n",
    "\n",
    "for clf in list_of_CLFs:\n",
    "    _ = clf.fit(X_train, y_train)\n",
    "    pred = clf.score(X_test, y_test)\n",
    "    name = str(type(clf)).split(\".\")[-1][:-2]\n",
    "    print(\"Acc: %0.5f for the %s\" % (pred, name))\n",
    "\n",
    "    ranking.append(pred)\n",
    "    list_of_CLFs_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "UJTr88Kge5bs",
    "outputId": "1870c805-26bc-49ba-ec54-ba59f075c9e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.95077 for the EnsembleVoteClassifier\n"
     ]
    }
   ],
   "source": [
    "eclf = EnsembleVoteClassifier(clfs=list_of_CLFs, refit=False, voting='soft')\n",
    "_ = eclf.fit(X_train, y_train)\n",
    "pred = eclf.score(X_test, y_test)\n",
    "print(\"Acc: %0.5f for the %s\" % (pred, str(type(eclf)).split(\".\")[-1][:-2]))\n",
    "\n",
    "\n",
    "pred = eclf.predict(X_test)\n",
    "probas = eclf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "id": "MU1Lvj7fVA9S"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "n = 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNu2XDAIjaQs"
   },
   "source": [
    "### Try RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "5UJPOp7yVBA9",
    "outputId": "ea02343d-37cf-4827-b7e8-b32a3e2caaf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new shape (206138, 10)\n",
      "Acc: 0.93705 for the DecisionTreeClassifier\n",
      "Acc: 0.95027 for the RandomForestClassifier\n",
      "Acc: 0.94846 for the ExtraTreesClassifier\n",
      "Acc: 0.93379 for the XGBClassifier\n",
      "Acc: 0.94804 for the LGBMClassifier\n",
      "Acc: 0.94902 for the EnsembleVoteClassifier\n"
     ]
    }
   ],
   "source": [
    "rfe = RFE(DecisionTreeClassifier(), n).fit(X_train, y_train)\n",
    "\n",
    "desiredIndices = np.where(rfe.support_==True)[0]\n",
    "whitelist = X_train.columns.values[desiredIndices]\n",
    "X_train_RFE, X_test_RFE = X_train[whitelist], X_test[whitelist]\n",
    "\n",
    "print('new shape', X_train_RFE.shape) \n",
    "\n",
    "for clf in list_of_CLFs:\n",
    "    _ = clf.fit(X_train_RFE,y_train)\n",
    "    pred = clf.score(X_test_RFE,y_test)\n",
    "    name = str(type(clf)).split(\".\")[-1][:-2]\n",
    "    print(\"Acc: %0.5f for the %s\" % (pred, name))\n",
    "\n",
    "    ranking.append(pred)\n",
    "    list_of_CLFs_names.append(name)\n",
    "\n",
    "\n",
    "eclf = EnsembleVoteClassifier(clfs=list_of_CLFs, refit=False, voting='soft')\n",
    "_ = eclf.fit(X_train_RFE, y_train)\n",
    "pred = eclf.score(X_test_RFE, y_test)\n",
    "print(\"Acc: %0.5f for the %s\" % (pred, str(type(eclf)).split(\".\")[-1][:-2]))\n",
    "\n",
    "\n",
    "pred = eclf.predict(X_test_RFE)\n",
    "probas = eclf.predict_proba(X_test_RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEl8uH1njcvP"
   },
   "source": [
    "### Try SVD and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "DL6SblkRVBEs",
    "outputId": "3ad71536-fddc-4f92-8784-f3d708681ceb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.87987 for the DecisionTreeClassifier\n",
      "Acc: 0.89908 for the RandomForestClassifier\n",
      "Acc: 0.89821 for the ExtraTreesClassifier\n",
      "Acc: 0.87024 for the XGBClassifier\n",
      "Acc: 0.88848 for the LGBMClassifier\n",
      "Acc: 0.89512 for the EnsembleVoteClassifier\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=n).fit(X_train)\n",
    "X_train_svd, X_test_svd = svd.transform(X_train), svd.transform(X_test)\n",
    "\n",
    "for clf in list_of_CLFs:\n",
    "    _ = clf.fit(X_train_svd, y_train)\n",
    "    pred = clf.score(X_test_svd, y_test)\n",
    "    name = str(type(clf)).split(\".\")[-1][:-2]\n",
    "    print(\"Acc: %0.5f for the %s\" % (pred, name))\n",
    "\n",
    "    ranking.append(pred)\n",
    "    list_of_CLFs_names.append(name)\n",
    "\n",
    "eclf = EnsembleVoteClassifier(clfs=list_of_CLFs, refit=False, voting='soft')\n",
    "_ = eclf.fit(X_train_svd, y_train)\n",
    "pred = eclf.score(X_test_svd, y_test)\n",
    "print(\"Acc: %0.5f for the %s\" % (pred, str(type(eclf)).split(\".\")[-1][:-2]))\n",
    "\n",
    "\n",
    "pred = eclf.predict(X_test_svd)\n",
    "probas = eclf.predict_proba(X_test_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "WT6trUnzJPk8",
    "outputId": "b426c234-279b-4765-a5ab-77ce5cbbd78b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.87896 for the DecisionTreeClassifier\n",
      "Acc: 0.89388 for the RandomForestClassifier\n",
      "Acc: 0.88905 for the ExtraTreesClassifier\n",
      "Acc: 0.86518 for the XGBClassifier\n",
      "Acc: 0.88235 for the LGBMClassifier\n",
      "Acc: 0.89176 for the EnsembleVoteClassifier\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=n).fit(X_train)\n",
    "X_train_pca, X_test_pca = pca.transform(X_train), pca.transform(X_test)\n",
    "\n",
    "for clf in list_of_CLFs:\n",
    "    _ = clf.fit(X_train_pca, y_train)\n",
    "    pred = clf.score(X_test_pca, y_test)\n",
    "    name = str(type(clf)).split(\".\")[-1][:-2]\n",
    "    print(\"Acc: %0.5f for the %s\" % (pred, name))\n",
    "\n",
    "    ranking.append(pred)\n",
    "    list_of_CLFs_names.append(name)\n",
    "\n",
    "eclf = EnsembleVoteClassifier(clfs=list_of_CLFs, refit=False, voting='soft')\n",
    "_ = eclf.fit(X_train_pca, y_train)\n",
    "pred = eclf.score(X_test_pca, y_test)\n",
    "print(\"Acc: %0.5f for the %s\" % (pred, str(type(eclf)).split(\".\")[-1][:-2]))\n",
    "\n",
    "\n",
    "pred = eclf.predict(X_test_pca)\n",
    "probas = eclf.predict_proba(X_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JApW6PtyluMN"
   },
   "source": [
    "# Lets' try another way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1pgtJb9jRk_"
   },
   "source": [
    "# MLP with L1 loss for feature selection  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Q5TVZ0RmKz96",
    "outputId": "4b20bbb2-9c47-4575-b89e-346cf8f1b880"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "np.unique(y_train)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "X05LsfcUjRyh",
    "outputId": "9f9c2511-0b6b-4ce3-dcbe-0e6f7f39ee5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: [0 1]\n"
     ]
    }
   ],
   "source": [
    "# device = 'cpu'\n",
    "input_size = 42\n",
    "hidden_size = 32 \n",
    "hidden_size_2 = 10\n",
    "num_classes = np.unique(y_train) # faster to code like a dumbass... len(set(y_train.values.flatten().tolist()))\n",
    "print(f'Number of classes: {np.unique(y_train)}')\n",
    "num_epochs = 5\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.input_size = input_size # ?? \n",
    "        self.l1 = nn.Linear(input_size, hidden_size) \n",
    "        self.l2 = nn.Linear(hidden_size, hidden_size_2)  \n",
    "        self.l3 = nn.Linear(hidden_size_2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.elu = nn.ELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        # no activation and no softmax at the end\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "id": "Po7VpfirjR2L",
    "outputId": "2fbe11f9-fec9-47d8-b3b3-ad85c679ea25"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16668/280846913.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfactor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.005\u001b[0m \u001b[1;31m#0.00005 # reg term coefficient/multiplier/weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Loss and optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16668/286693097.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_size, hidden_size, num_classes)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mELU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfactory_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "factor = 0.005 #0.00005 # reg term coefficient/multiplier/weight  \n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# L1 Regularizer\n",
    "l1_reg_criterion = nn.L1Loss() # size_average=False \n",
    "\n",
    "X_train_vals= X_train.values # _RFE\n",
    "y_train_vals = y_train.values.flatten()\n",
    "\n",
    "X_test_vals= X_test.values\n",
    "y_test_vals = y_test.values.flatten()\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # for i in range(len(X_train_RFE_vals)//100 + 1): #, batch_size\n",
    "\n",
    "    n_correct = 0\n",
    "    n_samples = 0 \n",
    "    for i in range(0, X_train_vals.shape[0], batch_size):\n",
    "\n",
    "        x = torch.as_tensor(X_train_vals[i:i+batch_size], dtype=torch.float).to(device)\n",
    "        y = torch.as_tensor(y_train_vals[i:i+batch_size], dtype=torch.long).to(device)\n",
    "        \n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        reg_loss = 0 \n",
    "        for name, param in model.l1.state_dict().items(): # L1 is the first layer \n",
    "          if name == 'weight':\n",
    "            # print(param.size())\n",
    "            # print((param-param).sum().item()); throw_for_bar_after_print_stop\n",
    "            reg_loss = torch.norm(model.l1.state_dict()['weight'], p=1).item() # l1_reg_criterion(param, param-param)\n",
    "        \n",
    "        # print(f'loss: {loss}, reg_loss: {reg_loss}'); throw_for_bar_after_print_stop\n",
    "\n",
    "        loss_pre = loss\n",
    "        loss = loss + (factor * reg_loss)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Train epoch accuracy\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        n_samples += y.size(0)\n",
    "        n_correct += (predicted == y).sum().item()\n",
    "    print(f'Epoch [{epoch}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}, Acc: {100.0 * n_correct / (n_samples+1):.4f}\\t loss: {loss_pre:.4f}, reg_loss: {factor * reg_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "r5SPcmAFjR5l",
    "outputId": "cef0f0ef-a4cc-4daf-e2be-7c96c0b66454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network: 63.88155852219808 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "X_test_vals= X_test.values # _RFE\n",
    "y_test_vals = y_test.values.flatten()\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0 \n",
    "    # for i in range(len(X_train_RFE_vals)//100 + 1):   \n",
    "    for i in range(0, X_test_RFE_vals.shape[0], batch_size):\n",
    "        x = torch.as_tensor(X_test_vals[i:i+batch_size], dtype=torch.float).to(device)\n",
    "        y = torch.as_tensor(y_test_vals[i:i+batch_size], dtype=torch.long).to(device)\n",
    "        \n",
    "        outputs = model(x)\n",
    "        if len(outputs.data) > 0:\n",
    "          # max returns (value ,index)\n",
    "          _, predicted = torch.max(outputs.data, dim=1)\n",
    "          \n",
    "          n_samples += y.size(0)\n",
    "          n_correct += (predicted == y).sum().item()\n",
    "\n",
    "        else:\n",
    "          print(\"what???\")\n",
    "          print(x, outputs.data)\n",
    "    acc = 100.0 * n_correct / (n_samples+1)\n",
    "    print(f'Accuracy of the network: {acc} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HMfAuoOjSAq"
   },
   "source": [
    "> Accuracy of the network: 79.643%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "o3yKHghIjSEV",
    "outputId": "77fc0cf6-57b5-4c5a-f745-ecf059f241c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLP] Top ten feature weights:  [0.01252026 0.00658208 0.02404366 0.04574006 0.01802584 0.02314406\n",
      " 0.02250805 0.04280177 0.01996318 0.03054257]\n",
      "[MLP] Top ten feature indexes:  [ 9 24 26 35  6 40 30  7 27 15]\n",
      "[MLP] Last ten feature indexes: [28  3 14 10 31 37 19 36 22 41]\n",
      "\n",
      "[DecisionTreeClassifier] Top ten feature indexes:  [ 9 24 26 35  6 40 30  7 27 15]\n",
      "[DecisionTreeClassifier] Last ten feature indexes: [28  3 14 31 10 36 22 37 19 41]\n"
     ]
    }
   ],
   "source": [
    "# print(model)\n",
    "# print(model.l1)\n",
    "# for name, param in model.l1.state_dict().items(): # L1 is the first layer \n",
    "#   if name == 'weight': \n",
    "#     for param_ in param.T:\n",
    "#       print(param_.sum().item())\n",
    "\n",
    "weights = [weight.sum().item() for weight in model.l1.state_dict()['weight'].T]\n",
    "weights = [round(abs(i), 5) for i in weights]\n",
    "weights = np.array(weights/np.sum(weights))\n",
    "weights_idx = np.argsort(fi)[::-1]\n",
    "\n",
    "\n",
    "print(f\"[MLP] Top ten feature weights:  {weights[:10]}\")\n",
    "print(f\"[MLP] Top ten feature indexes:  {weights_idx[:10]}\")\n",
    "print(f\"[MLP] Last ten feature indexes: {weights_idx[-10:]}\\n\")\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "_ = clf.fit(X_train, y_train)\n",
    "fi = [round(i, 5 ) for i in clf.feature_importances_] # round \n",
    "fi = np.array(fi) # to array \n",
    "fi = fi/np.sum(fi) # ensure it's normalize\n",
    "fi_idx = np.argsort(fi)[::-1] # from largest to smallest\n",
    "\n",
    "# print(fi[fi_idx])\n",
    "print(f\"[DecisionTreeClassifier] Top ten feature indexes:  {fi_idx[:10]}\")\n",
    "print(f\"[DecisionTreeClassifier] Last ten feature indexes: {fi_idx[-10:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuaTFpTeCgG0"
   },
   "source": [
    "```\n",
    "factor = 0.0\n",
    "[MLP] Top ten feature indexes:  [ 9 24 26 35  6 40 30  7 27 15]\n",
    "[MLP] Top ten feature weights:  [0.06196863 0.02227446 0.04971818 0.02841482 0.04204971 0.02587458 0.04431207 0.02130464 0.01099255 0.00912548]\n",
    "[MLP] Last ten feature indexes: [ 5  3 14 28 36 10 37 19 22 41]\n",
    "```\n",
    "\n",
    "```\n",
    "factor = 0.00005\n",
    "[MLP] Top ten feature indexes:  [ 9 24 26 35  6 40 30  7 27 15]\n",
    "[MLP] Top ten feature weights:  [0.03054257 0.0136881  0.03092274 0.00804256 0.02250805 0.006705 0.00787411 0.04280177 0.05547793 0.03282484]\n",
    "[MLP] Last ten feature indexes: [31  3  5 14 37 36 10 19 22 41]\n",
    "```\n",
    "Overkill... 0.0005 is too high\n",
    "```\n",
    "Epoch [1/5], Step [206129/206138], Loss: 0.6786, Acc: 72.8474\t loss: 0.4932, reg_loss: 0.18541\n",
    "Epoch [3/5], Step [206129/206138], Loss: 0.7730, Acc: 65.8260\t loss: 0.5133, reg_loss: 0.25978\n",
    "\n",
    "factor = 0.0005\n",
    "[MLP] Top ten feature indexes:  [ 9 24 26 35  6 40 30  7 27 15]\n",
    "[MLP] Top ten feature weights:  [0.04053201 0.0279744  0.03267631 0.00767643 0.0375806  0.00558694 0.01319869 0.04616321 0.00876322 0.01025393]\n",
    "[MLP] Last ten feature indexes: [ 5  3 14 31 10 36 37 19 22 41]\n",
    "```\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Doesn't seem to be very useful here. Maybe its because we have **A lot** of data.\n",
    "\n",
    "* Too bad I made these chances by hand. I could explore more\n",
    "* I should have set the seed\n",
    "* Future Ali askes that i should tried a higher `lambda`, whch in my code is `factor = 0.00005`. Now in 2022 this code fails to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "id": "ixQ7uR00jSIB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "id": "4OHfbx1qjSLS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "id": "twvrr6d5jSOq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8gsbiYtjSRk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1T_uPrDSjRop"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmvwsp-KnNWi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJY4PEQhnNb5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PkbExAc-nNfp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2aaz0iLnNja"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v30s7FidnNm-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARG5tDbjnNqa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74QP1VY6nNth"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mohA8IeSnNw7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-Jf6wmTnN0B"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVGSOxgunN3S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkcSDGFhnN6H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMKSBLJknN8i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuYFI1JRJt6k"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "UNSW_NB15 - PyTorch feature selection via L1 regularization on layer_1 ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
